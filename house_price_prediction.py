# -*- coding: utf-8 -*-
"""House_price_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t49jNnApZ46Pd_uvLUOaHrDGyzL9EVvs
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)

"""## Load Datasets"""

import pandas as pd
from google.colab import files

uploaded = files.upload()

for fn in uploaded.keys():
  print('User uploaded file "{name}" with length {length} bytes'.format(
      name=fn, length=len(uploaded[fn])))

uploaded

import io

df1 = pd.read_csv(io.StringIO(uploaded['bengaluru_house_prize.csv'].decode('utf-8')))

df1.head()

df1.shape   #shows no odf rows and col

#print count of data sample of each type
df1.groupby('area_type')['area_type'].agg('count')

# dropping some columns to have simple data set
df2 = df1.drop(['area_type','society','balcony','availability'],axis='columns')
df2.head()

"""## Data Cleaning"""

#to count how many null values are present
df2.isnull().sum()

#we can also fill the null values by taking the mean of each individual row and placing mean value in resp rows of that column
# here we drop all the rows having one or more than one null value
df3 = df2.dropna()
df3.isnull().sum()

df3.isnull()

df3.shape

df3['size'].unique()   #BHK and Bedroom have same meaning (column named size)

# create a nwe column named bhk
# split the strings '4 Bedroom' or '2 BHK' into two by space between them and put the string present at index 0 into the column
#A lambda function can take any number of arguments, but can only have one expression.

df3['bhk']=df3['size'].apply(lambda x:int(x.split(" ")[0]))

df3.head()

df3['bhk'].unique()

df3[df3.bhk>20]

df3.total_sqft.unique()  #we need to such ranges not the single  value so performing below function

def is_float(x):
  try:
      float(x)
  except:
      return False
  return True

#to get the total_sqft column values inform of range
df3[~df3['total_sqft'].apply(is_float)].head(10)

#total_sqft column still sontainsattributes like 34.46Sq. Meter ,4125Perch
#incomming data is not uniformed , it is unstructured, it has outliers and data errors

#python function that takes range as input and returns the average value and remove rows containing 4125Perch such values
def convert_sqft_to_num(x):
  tokens = x.split('-')
  if len(tokens)==2:
    return (float(tokens[0])+float(tokens[1]))/2
  try:
      return float(x)
  except:
      return None

convert_sqft_to_num('2166')

convert_sqft_to_num('3067 - 8156')

convert_sqft_to_num('34.46Sq. Meter')

#to get the total_sqft column values inform of float
df4 = df3.copy()
df4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)
df4.head(5)

df4.loc[30]

(2100+2850)/2

"""## feature reduction and dimensionality reduction techquines"""

df4.head(3)

#making price per square feet column
df5 = df4.copy()
df5['price_per_sqft'] = df5['price']*100000/df5['total_sqft']
df5.head()

#checking unique no of loaction
df5.location.unique()

#checking  no ofunique loaction
len(df5.location.unique())

# to remove the leaading space or space at the end of the location
df5.location = df5.location.apply(lambda x: x.strip())

#create a  variable location_stats and get the aggregate count of that location and sort them in asecending order
location_stats = df5.groupby('location')['location'].agg('count').sort_values(ascending=False)
location_stats

#to know how many location have less than 10 data stats
len(location_stats[location_stats<=10])

#so out of 1293 we have 1052 data_location that has less than equalto$ 10 data stats

location_stats_less_than_10 = location_stats[location_stats<=10]
location_stats_less_than_10

#TOTAL NO OF UNIQUE LOCATIONS
len(df5.location.unique())

df5.location = df5.location.apply(lambda x: 'other' if x in location_stats_less_than_10 else x)
len(df5.location.unique())

df5.head(10)

"""## Outlier detection and outlier removal 
ouliers represent the extreme variations in the data set
"""

#it shows the outliers as comparision of total_sqft and bhk is not possible
#removing th rows having outliers
df5[df5.total_sqft/df5.bhk<300].head(20)

df5.shape

#negate the outliers
#we will get the table values that doesnot contains outlier values
df6 = df5[~(df5.total_sqft/df5.bhk<300)]
df6.shape

#describe column gives some basic staticstics on that particular column
df6.price_per_sqft.describe()

#write a function that can remove the extreme cases using STANDARD DEVIATION
#per location find mean and standard deviation and filter the data points that  are beyond standard deviation
def remove_pps_outliers(df):
    df_out = pd.DataFrame()
    for key, subdf in df.groupby('location'):
        m = np.mean(subdf.price_per_sqft)
        st = np.std(subdf.price_per_sqft)
        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]
        df_out = pd.concat([df_out,reduced_df],ignore_index=True)
    return df_out
df7 = remove_pps_outliers(df6)
df7.shape

# display a scatter plot that shows 2bhk and 3 bhk apprtment for same location
def plot_scatter_chart(df,location):
  bhk2 = df[(df.location == location) & (df.bhk == 2)]
  bhk3 = df[(df.location == location) & (df.bhk == 3)]
  matplotlib.rcParams['figure.figsize'] = (15,10)
  plt.scatter(bhk2.total_sqft,bhk2.price_per_sqft, color='blue',label='2 BHK',s=50)
  plt.scatter(bhk3.total_sqft,bhk3.price_per_sqft,marker='+', color='green',label='3 BHK',s=50)
  plt.xlabel('Total Square Feet Area')
  plt.ylabel('Price per Square Feet')
  plt.title(location)
  plt.legend()

plot_scatter_chart(df7,"Rajaji Nagar")

plot_scatter_chart(df7,"Hebbal")

"""**here the blue data points are higher(above) than the green data points ie:  2bhk > 3bhk price ,  so to remove such outliers**

3 bedroom apartment is less than 2 bedroom apartment (with same square ft area). For a given location, we will build a dictionary of stats per bhk, i.e.


```

{
    '1': {
        'mean': 4000,
        'std: 2000,
        'count': 34
    },
    '2' : {
        'mean': 4300,
        'std: 2300,
        'count': 22
    },    
}
```

We can remove those 2 BHK apartments whose price_per_sqft is less than mean price_per_sqft of 1 BHK apartment
"""

def remove_bhk_outliers(df):
    exclude_indices = np.array([])
    for location, location_df in df.groupby('location'):
        bhk_stats = {}
        for bhk, bhk_df in location_df.groupby('bhk'):
            bhk_stats[bhk] = {
                'mean': np.mean(bhk_df.price_per_sqft),
                'std': np.std(bhk_df.price_per_sqft),
                'count': bhk_df.shape[0]
            }
        for bhk, bhk_df in location_df.groupby('bhk'):
            stats = bhk_stats.get(bhk-1)
            if stats and stats['count']>5:
                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)
    return df.drop(exclude_indices,axis='index')
df8 = remove_bhk_outliers(df7)
# df8 = df7.copy()
df8.shape

plot_scatter_chart(df8,"Hebbal")

# see the histogram and seehow many properties we have in per square feet area
import matplotlib
matplotlib.rcParams['figure.figsize'] = (20,10)
plt.hist(df8.price_per_sqft,rwidth=0.8)
plt.xlabel("price per square feet")
plt.ylabel("count")

df8.bath.unique()

df8[df8.bath>10]

plt.hist(df8.bath,rwidth=0.8)
plt.xlabel("no of bathrooms")
plt.ylabel("count")

#if we have (no of bathrooms )= (no of bedrooms + 2) then remove that outlier
df8[df8.bath>df8.bhk+2]

df9 = df8[df8.bath<df8.bhk+2]
df9.shape

"""now we can train our machine learning model
we can drop price_per_sqft cloumn (it was onl used to detect outliers)
size col can also be dropped
"""

df10 = df9.drop(['size','price_per_sqft'],axis = 'columns')

df10.head()

"""Now we will use machine learning model and build k-fold cross validation and grid search cv to come up with the best algorithm as well as the best paramater

Machine learning model cannot interpret the text data, it can only interpret the numeric values. We need to convert location column(text/categorical col) into numeric column.

One way to convert categorical column into numericcolumn is to to use hot encoding also called as dummies
"""

#using pandas dummies method
dummies = pd.get_dummies(df10.location)
dummies.head()

# concatenate df10 and the above dummies
# and drop the last column named others
df11 = pd.concat([df10,dummies.drop('other',axis = 'columns')],axis = 'columns')
df11.head()

df12 = df11.drop('location',axis = 'columns')
df12.head()

df12.shape

# price is the dependent variable so we need to drop it
X = df12.drop('price',axis = 'columns')
X.head()
# now we will have all independent variables

y = df12.price
y.head()

"""here we will divide our data set into two ie training data set and test data set"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2,random_state = 10)

from sklearn.linear_model import LinearRegression
lr_clf = LinearRegression()
lr_clf.fit(X_train,y_train)
lr_clf.score(X_test,y_test)

#using k fold cross validation
from sklearn.model_selection import ShuffleSplit
from sklearn.model_selection import  cross_val_score

cv = ShuffleSplit(n_splits = 5, test_size = 0.2, random_state = 0) #it will randomize the sample so that each of ford will have equal distribution

cross_val_score(LinearRegression(), X, y, cv=cv)

from sklearn.model_selection import GridSearchCV

from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor

def find_best_model_using_gridsearchcv(X,y):
    algos = {
        'linear_regression' : {
            'model': LinearRegression(),
            'params': {
                'normalize': [True, False]
            }
        },
        'lasso': {
            'model': Lasso(),
            'params': {
                'alpha': [1,2],
                'selection': ['random', 'cyclic']
            }
        },
        'decision_tree': {
            'model': DecisionTreeRegressor(),
            'params': {
                'criterion' : ['mse','friedman_mse'],
                'splitter': ['best','random']
            }
        }
    }
    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)  #5 split
    for algo_name, config in algos.items():
        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        gs.fit(X,y)
        scores.append({
            'model': algo_name,
            'best_score': gs.best_score_,
            'best_params': gs.best_params_
        })

    return pd.DataFrame(scores,columns=['model','best_score','best_params'])

find_best_model_using_gridsearchcv(X,y)  #calling Xand y'

"""from the above results we can say linear regression gives best score"""

X.columns

np.where(X.columns == '1st Phase JP Nagar')[0][0]  #it will return the index

# this will give the predicted price

def predict_price(location,sqft,bath,bhk):    
    loc_index = np.where(X.columns==location)[0][0]

    x = np.zeros(len(X.columns))
    x[0] = sqft
    x[1] = bath
    x[2] = bhk
    if loc_index >= 0:
        x[loc_index] = 1

    return lr_clf.predict([x])[0]

#  check for property sq feet = 1000 , bedroom = 2, bathroom = 2
predict_price('1st Phase JP Nagar',1000, 3, 3)

predict_price('1st Phase JP Nagar',1000, 2, 2)

predict_price('Thanisandra',1000, 2, 2)

predict_price('Thanisandra',1000, 3, 3)

"""**Now our model building procedure is done . bold text
Export the model to a pickle file and then it will be used by our python flask serve**
"""

import pickle
with open('bengaluru_house_price_model.pickle','wb') as f:
    pickle.dump(lr_clf,f)

import json
columns = {
    'data_columns' : [col.lower() for col in X.columns]
}
with open("columns.json","w") as f:
  f.write(json.dumps(columns))

